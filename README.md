# wer-sigtest
Script to perform statistical significance test between ASR (Automatic Speech Recognition) transcription hypotheses. This can be used to evaluate whether differences in WER (word error rate) are actually significant or not.

## Install

You will need to use the comands `sclite` and `sc_stats` from the NIST Scoring Toolkit available here:
http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sctk.htm


## Files

**RUN.sh** contains an example script to (1) generate an SGML (XML-like) file for transcript hypotheses you want to compare, and (2) to compare the hypotheses using a statistical significance test of your choice. The repo contains:

- **ref.trn** : A _reference_ (ground truth) transcript in the format of <transcript line> <unique line ID>.
- **hyp.A.trn** and **hyp.B.trn** : Two transcript _hypthoses_ each generated by different ASR setups.
  
### Generate SGML file.

Run the following command for hypothesis A and B.

```
sclite -F -i wsj -r ref.trn -h hyp.A.trn -o sgml
```

```
sclite -F -i wsj -r ref.trn -h hyp.B.trn -o sgml
```

### Compare the hyptheses

```
cat hyp.A.trn.sgml hyp.B.trn.sgml | sc_stats -p -t mapsswe -v -u -n result.A-B.mapsswe
```

### Statistical Significance Tests Available
Instead of the `mapsswe` (Matched Pairs Sentence-Segment Word Error) option, you can use `mcn` (McNemar), `sign`, `wilc` (Wilcoxon Signed Rank ), `anovar` (Analysis of Variance), `std4` (standard four - mcn, mapsswe, wilc, and sign)
